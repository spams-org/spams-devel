<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hevea 2.06">

<META name="Author" content="Julien Mairal">
<link rel="stylesheet" href="doc_spams.css"><link rel="stylesheet" type="text/css" href="doc_spams.css">
<title>Introduction</title>
</head>
<body>
<a href="index.html"><img src="contents_motif.gif" alt="Up"></a>
<a href="doc_spams002.html"><img src="next_motif.gif" alt="Next"></a>
<hr>
<h2 class="section" id="sec1">1  Introduction</h2>
<p>
SPAMS (SPArse Modeling Software) is an open-source optimization toolbox under
licence GPLv3. It implements algorithms for solving various machine learning
and signal processing problems involving sparse regularizations.</p><p>The library is coded in C++, is compatible with Linux, Mac, and Windows 32bits
and 64bits Operating Systems. It is interfaced with Matlab, but can be called
from any C++ application. A R and Python interface has been developed by
Jean-Paul Chieze.</p><p>It requires an implementation of BLAS and LAPACK for performing efficient
linear algebra operations such as the one provided by matlab/R, atlas, netlib,
or the one provided by Intel (Math Kernel Library). It also exploits
multi-core CPUs when this feature is supported by the compiler, through
OpenMP.</p><p>The current licence is GPLv3 available at
<span class="c001">http://www.gnu.org/licenses/gpl.html</span>, which limits its usage. For other
usages (such as the use in proprietary softwares), please contact the author.</p><p>Version 2.2 of SPAMS is divided into three main “toolboxes” and has a few
additional miscellaneous functions:
</p><ul class="itemize"><li class="li-itemize">
The <span class="c007">Dictionary learning and matrix factorization toolbox</span>
contains the online learning technique of [<a href="doc_spams008.html#mairal7">18</a>, <a href="doc_spams008.html#mairal9">19</a>] and its
variants for solving various matrix factorization problems:
<ul class="itemize"><li class="li-itemize">
Dictionary Learning for sparse coding.
</li><li class="li-itemize">Sparse principal component analysis.
</li><li class="li-itemize">Non-negative matrix factorization.
</li><li class="li-itemize">Non-negative sparse coding.
</li></ul>
</li><li class="li-itemize">The <span class="c007">Sparse decomposition toolbox</span> contains efficient implementations of
<ul class="itemize"><li class="li-itemize">
Orthogonal Matching Pursuit, (or Forward Selection) [<a href="doc_spams008.html#weisberg">28</a>, <a href="doc_spams008.html#mallat4">21</a>].
</li><li class="li-itemize">The LARS/homotopy algorithm [<a href="doc_spams008.html#efron">8</a>] (variants for solving Lasso and Elastic-Net problems).
</li><li class="li-itemize">A weighted version of LARS. 
</li><li class="li-itemize">OMP and LARS when data come with a binary mask.
</li><li class="li-itemize">A coordinate-descent algorithm for ℓ<sub>1</sub>-decomposition problems [<a href="doc_spams008.html#fu">10</a>, <a href="doc_spams008.html#friedman">9</a>, <a href="doc_spams008.html#wu">29</a>]. 
</li><li class="li-itemize">A greedy solver for simultaneous signal approximation as defined in [<a href="doc_spams008.html#tropp2">27</a>, <a href="doc_spams008.html#tropp3">26</a>] (SOMP).
</li><li class="li-itemize">A solver for simulatneous signal approximation with ℓ<sub>1</sub>/ℓ<sub>2</sub>-regularization based on block-coordinate descent.
</li><li class="li-itemize">A homotopy method for the Fused-Lasso Signal Approximation as defined in [<a href="doc_spams008.html#friedman">9</a>] with the homotopy method presented in the appendix of [<a href="doc_spams008.html#mairal9">19</a>].
</li><li class="li-itemize">A tool for projecting efficiently onto a few convex sets
inducing sparsity such as the ℓ<sub>1</sub>-ball using the method of
[<a href="doc_spams008.html#brucker">3</a>, <a href="doc_spams008.html#maculan">16</a>, <a href="doc_spams008.html#duchi">7</a>], and Elastic-Net or Fused Lasso constraint sets as
proposed in the appendix of [<a href="doc_spams008.html#mairal9">19</a>].
</li></ul>
</li><li class="li-itemize">The <span class="c007">Proximal toolbox</span>: An implementation of proximal methods
(ISTA and FISTA [<a href="doc_spams008.html#beck">1</a>]) for solving a large class of sparse approximation
problems with different combinations of loss and regularizations. One of the main
features of this toolbox is to provide a robust stopping criterion based on
<em>duality gaps</em> to control the quality of the optimization, whenever
possible. It also handles sparse feature matrices for large-scale problems. The following regularizations are implemented:
<ul class="itemize"><li class="li-itemize">
Tikhonov regularization (squared ℓ<sub>2</sub>-norm).
</li><li class="li-itemize">ℓ<sub>1</sub>-norm, ℓ<sub>2</sub>, ℓ<sub>∞</sub>-norms.
</li><li class="li-itemize">Elastic-Net [<a href="doc_spams008.html#zou">31</a>].
</li><li class="li-itemize">Fused Lasso [<a href="doc_spams008.html#tibshirani2">25</a>].
</li><li class="li-itemize">Tree-structured sum of ℓ<sub>2</sub>-norms (see [<a href="doc_spams008.html#jenatton3">13</a>, <a href="doc_spams008.html#jenatton4">14</a>]).
</li><li class="li-itemize">Tree-structured sum of ℓ<sub>∞</sub>-norms (see [<a href="doc_spams008.html#jenatton3">13</a>, <a href="doc_spams008.html#jenatton4">14</a>]).
</li><li class="li-itemize">General sum of ℓ<sub>∞</sub>-norms (see [<a href="doc_spams008.html#mairal10">20</a>]).
</li><li class="li-itemize">Mixed ℓ<sub>1</sub>/ℓ<sub>2</sub>-norms on matrices [<a href="doc_spams008.html#yuan">30</a>, <a href="doc_spams008.html#obozinski">23</a>].
</li><li class="li-itemize">Mixed ℓ<sub>1</sub>/ℓ<sub>∞</sub>-norms on matrices [<a href="doc_spams008.html#yuan">30</a>, <a href="doc_spams008.html#obozinski">23</a>].
</li><li class="li-itemize">Mixed ℓ<sub>1</sub>/ℓ<sub>2</sub>-norms on matrices plus ℓ<sub>1</sub> [<a href="doc_spams008.html#sprechmann">24</a>].
</li><li class="li-itemize">Mixed ℓ<sub>1</sub>/ℓ<sub>∞</sub>-norms on matrices plus ℓ<sub>1</sub>.
</li><li class="li-itemize">group-lasso with ℓ<sub>2</sub> or ℓ<sub>∞</sub>-norms.
</li><li class="li-itemize">group-lasso+ℓ<sub>1</sub>.
</li><li class="li-itemize">multi-task tree-structured sum of ℓ<sub>∞</sub>-norms (see [<a href="doc_spams008.html#mairal10">20</a>]).
</li><li class="li-itemize">trace norm.
</li><li class="li-itemize">ℓ<sub>0</sub> pseudo-norm (only with ISTA)
</li><li class="li-itemize">Tree-structured ℓ<sub>0</sub> (only with ISTA)
</li><li class="li-itemize">rank regularization for matrices (only with ISTA)
</li></ul>
All of these regularization functions can be used with the following losses
<ul class="itemize"><li class="li-itemize">
square loss.
</li><li class="li-itemize">square loss with missing observations.
</li><li class="li-itemize">logistic loss, weighted logistic loss.
</li><li class="li-itemize">multi-class logistic
</li></ul>
This toolbox can also enforce positivity constraints and handles sparse matrices.
</li><li class="li-itemize">A few tools for performing linear algebra operations such as a
conjugate gradient algorithm, and manipulating sparse matrices.
</li></ul><p>The toolbox was written by Julien Mairal when he was at INRIA, with the
collaboration of Francis Bach (INRIA), Jean Ponce (Ecole Normale Supérieure),
Guillermo Sapiro (University of Minnesota) and Rodolphe Jenatton (INRIA).</p><p>A R and Python interface has been written by Jean-Paul Chieze (INRIA), and a
few contributors have helped us making compilation scripts for various platforms.</p>
<hr>
<a href="index.html"><img src="contents_motif.gif" alt="Up"></a>
<a href="doc_spams002.html"><img src="next_motif.gif" alt="Next"></a>
</body>
</html>
